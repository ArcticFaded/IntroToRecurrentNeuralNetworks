{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For this workshop we will be using Keras to quickly prototype a working stock prediction model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from io import StringIO\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, math\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "ticker = 'AAPL'\n",
    "\n",
    "r = requests.get(\"https://finance.google.com/finance/historical?q=\" + ticker + \"&startdate=01-Jan-2008&output=csv\")\n",
    "stock = pd.read_csv(StringIO(r.text))\n",
    "\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Date Feature in time series serves as indexing\n",
    "\n",
    "## For style purposes the column we are trying to predict is always the last one(s)\n",
    "\n",
    "### Although here we are only predict closing price, recurrent neural networks are capable of predicting all the features in the dataset\n",
    "\n",
    "* unlike a traditional classification problem, the target column we are trying to predict is located within the training set\n",
    "* In the LSTM model, we are trying to use previous input sequences to try and predict future output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "cols = stock.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "stock = stock[cols]\n",
    "\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When using any algorithm that uses an activation function, your data must be normalized to values within the activation\n",
    "\n",
    "## For using the ReLu function, you use min-max scaler (values between 0 and 1)\n",
    "\n",
    "## For using the Tanh function, you use StandardScaler (values between -1 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizaing data\n",
    "scale = MinMaxScaler(feature_range=(0,1)) # or StandardScaler\n",
    "#scale = StandardScaler()\n",
    "price = MinMaxScaler(feature_range=(0,1))\n",
    "price.fit(stock['Close'].reshape(-1,1))\n",
    "stock = pd.DataFrame(scale.fit_transform(stock), columns=['Volume', \n",
    "                                                          'Open', \n",
    "                                                          'High', \n",
    "                                                          'Low', \n",
    "                                                          'Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing our data as a sequential model\n",
    "\n",
    "## If we have data like\n",
    "\n",
    "Volume | Open | High | Low | Close\n",
    "--- | --- | --- | --- | ---\n",
    "0.3106 | 0.1019 | 0.1004 | 0.0978 | 0.0991\n",
    "0.2393 | 0.0987 | 0.0979 | 0.0979 | 0.0992\n",
    "0.4237 | 0.0953 | 0.0942 | 0.0861 | 0.0866\n",
    "0.3547 | 0.0436 | 0.0416 | 0.0388 | 0.0397\n",
    "\n",
    "## suppose we choose to learn in sequences of 2 then our data will be represented like this\n",
    "\n",
    "[0.3106 | 0.1019 | 0.1004 | 0.0978 | 0.0991\n",
    "\n",
    "0.2393 | 0.0987 | 0.0979 | 0.0979 | 0.0992],\n",
    "\n",
    "[0.2393 | 0.0987 | 0.0979 | 0.0979 | 0.0992\n",
    "\n",
    "0.4237 | 0.0953 | 0.0942 | 0.0861 | 0.0866],\n",
    "\n",
    "[0.4237 | 0.0953 | 0.0942 | 0.0861 | 0.0866\n",
    "\n",
    "0.3547 | 0.0436 | 0.0416 | 0.0388 | 0.0397]\n",
    "\n",
    "we'll create a new object which is 3x2x5, [amount_of_sequences, sequence_length, amount_of_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "prices = {}\n",
    "\n",
    "def load_data(stock, seq_len, split):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.as_matrix()\n",
    "    sequence_length = seq_len + 1\n",
    "    result = []\n",
    "    \n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    \n",
    "    result = np.array(result)\n",
    "    row = len(result) * split\n",
    "    train = result[:int(row), :]\n",
    "    x_train = train[:, :-1]\n",
    "    y_train = train[:, -1][:,-1]\n",
    "    x_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1]\n",
    "    \n",
    "    \n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], amount_of_features))\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], amount_of_features))  \n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Recurrent Neural Network?\n",
    "\n",
    "![alt text](recurrent_cell.PNG \"Title\")\n",
    "\n",
    "Unlike regular feed foward networks, the output of a layer in a recurrent neural network feeds back in on itself\n",
    "\n",
    "![alt text](recurrent_network.PNG \"Title\")\n",
    "\n",
    "The problem with recurrent neural networks however is that the weight update is a function that grows exponential, meaning that updating weights suffer from something known as vanishing gradient\n",
    "\n",
    "![alt text](vanashing_RNN.PNG \"Title\")\n",
    "\n",
    "Popularity of Recurrent Neural Networks has been resugring thanks to the invention of this new architecture\n",
    "\n",
    "![alt text](LSTM_cell.PNG \"Title\")\n",
    "\n",
    "LSTM solves the issue of vanishing gradients by making linear changes to the output C_t, while keeping the change within the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(layers):\n",
    "    model = Sequential()\n",
    "\n",
    "    for x in range(0,1):\n",
    "        model.add(LSTM(input_dim=layers[0], output_dim=layers[1], return_sequences=True))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(layers[2], return_sequences=False)) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(output_dim=layers[2]))\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\",metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model([5, window, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4), dpi=100)\n",
    "\n",
    "plt.plot(pred, color='red', label='predicted price')\n",
    "plt.plot(y_test, color='black', label='read price')\n",
    "\n",
    "\n",
    "plt.xlabel('number of days where 01-Jan-2008 is 0 ')\n",
    "plt.ylabel('price per stock')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
